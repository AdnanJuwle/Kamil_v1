# AI Coding Assistant

A local LLM-powered coding assistant that helps you generate Python code using Ollama. This is a command-line tool that uses a local language model to generate, modify, and execute Python code.

## Features

- ğŸ¤– **Local LLM Integration**: Uses Ollama for private, local code generation
- ğŸ“ **Code Generation**: Generate Python code from natural language instructions
- ğŸ“„ **File Context**: Provide existing code files as context for modifications
- ğŸ’¾ **Save & Execute**: Save generated code and optionally run it immediately
- ğŸ”’ **Security**: Input validation and path sanitization to prevent security issues
- ğŸªŸ **Cross-Platform**: Works on Windows, macOS, and Linux

## Prerequisites

1. **Python 3.7+** installed on your system
2. **Ollama** installed and configured
   - Download from: https://ollama.ai
   - Ensure the `ollama` command is in your system PATH
   - Pull a model (default: `mistral:latest`):
     ```bash
     ollama pull mistral:latest
     ```

## Installation

1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd Kamil_v1
   ```

2. No additional Python packages are required (uses only standard library)

3. Verify Ollama is working:
   ```bash
   ollama list
   ```

## Configuration

You can configure the assistant using environment variables:

- `OLLAMA_MODEL`: The Ollama model to use (default: `mistral:latest`)
- `OLLAMA_TIMEOUT`: Timeout in seconds for Ollama requests (default: `300`)

Example:
```bash
# Linux/macOS
export OLLAMA_MODEL="llama2:latest"
export OLLAMA_TIMEOUT=600

# Windows PowerShell
$env:OLLAMA_MODEL="llama2:latest"
$env:OLLAMA_TIMEOUT=600
```

Or edit `config.py` directly to change defaults.

## Usage

Run the assistant:
```bash
python main.py
```

### Basic Workflow

1. Enter your coding instruction when prompted:
   ```
   You: create a function to calculate fibonacci numbers
   ```

2. Optionally provide a file for context:
   ```
   Do you want to provide a file for context? (y/n): y
   Enter file path: existing_code.py
   ```

3. Review the generated code

4. Save to file (optional):
   ```
   ğŸ’¾ Save to file? (y/n): y
   ğŸ“„ Filename (e.g., tool.py): fibonacci.py
   ```

5. Run the file (optional):
   ```
   â–¶ï¸ Run this file? (y/n): y
   ```

### Example Session

```
ğŸ“ AI Coding Assistant (type 'exit' or 'quit' to quit)

You: make a calculator python script
Do you want to provide a file for context? (y/n): n
ğŸ¤– Thinking...

ğŸ§  Plan:
def calculate(operation, num1, num2):
    if operation == '+':
        return num1 + num2
    elif operation == '-':
        return num1 - num2
    elif operation == '*':
        return num1 * num2
    elif operation == '/':
        return num1 / num2
    else:
        raise ValueError('Invalid operation')

ğŸ’¾ Save to file? (y/n): y
ğŸ“„ Filename (e.g., tool.py): calculator.py
âœ… Saved to calculator.py
â–¶ï¸ Run this file? (y/n): n
```

## Dataset Support

The assistant automatically replaces common dataset names with their full HuggingFace paths:
- `iris` â†’ `scikit-learn/iris`
- `mnist` â†’ `mnist`
- `imdb` â†’ `imdb`
- `ag_news` â†’ `ag_news`
- `emotion` â†’ `dair-ai/emotion`
- `yelp` â†’ `yelp_polarity`

## Project Structure

```
Kamil_v1/
â”œâ”€â”€ main.py              # Entry point
â”œâ”€â”€ agent.py             # CodingAgent class
â”œâ”€â”€ config.py            # Configuration settings
â”œâ”€â”€ file_ops.py          # File operations (save, execute)
â”œâ”€â”€ dataset_utils.py     # Dataset name replacements
â”œâ”€â”€ prompt_templates.py  # LLM prompt templates
â”œâ”€â”€ requirements.txt     # Dependencies (none required)
â”œâ”€â”€ README.md           # This file
â””â”€â”€ testresults/        # Generated test files
```

## Security Features

- **Path Sanitization**: Prevents directory traversal attacks
- **Input Validation**: Validates all user inputs
- **Safe Execution**: Uses subprocess instead of os.system
- **File Encoding**: Explicit UTF-8 encoding for cross-platform compatibility

## Troubleshooting

### "ollama command not found"
- Ensure Ollama is installed and in your PATH
- Verify with: `ollama --version`

### "Model not found"
- Pull the model: `ollama pull mistral:latest`
- Or change `MODEL_NAME` in `config.py`

### Timeout errors
- Increase `TIMEOUT_SECONDS` in `config.py` or set `OLLAMA_TIMEOUT` environment variable
- Check if your model is too slow for your hardware

### Permission errors
- Ensure you have write permissions in the current directory
- Check file paths are valid and accessible

## Contributing

This is an early version. Contributions and improvements are welcome!

## License

[Add your license here]

## Notes

- This is the first version (v1) - expect rough edges
- The assistant generates code but doesn't guarantee correctness
- Always review generated code before using in production
- Generated files are saved in the current working directory

